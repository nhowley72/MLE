{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stocks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 188\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# evaluate_model()\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# print(\"Pipeline complete!\")\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSFT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGOOGL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMZN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMETA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNVDA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTSLA\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 188\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 180\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(ticker)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(ticker):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# print(\"Starting the stock data pipeline...\")\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[43mdownload_and_clean_stock_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     preprocess_data(ticker)\n\u001b[1;32m    182\u001b[0m     train_model(ticker)\n",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m, in \u001b[0;36mdownload_and_clean_stock_data\u001b[0;34m(stock)\u001b[0m\n\u001b[1;32m     30\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     31\u001b[0m     data\u001b[38;5;241m.\u001b[39mto_csv(output_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stock \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstocks\u001b[49m:\n\u001b[1;32m     34\u001b[0m     data \u001b[38;5;241m=\u001b[39m yf\u001b[38;5;241m.\u001b[39mdownload(stock, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2015-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstock\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stocks' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import joblib\n",
    "\n",
    "def download_and_clean_stock_data(stock):\n",
    "    \"\"\"\n",
    "    Download stock data for the Magnificent Seven, clean it, and save as CSV files.\n",
    "    \"\"\"\n",
    "   \n",
    "    output_dir = \"raw_stock_data\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def save_cleaned_data(data, stock, output_path):\n",
    "        # Reset index to turn the Date index into a column\n",
    "        data.reset_index(inplace=True)\n",
    "        # Retain only the desired columns\n",
    "        data = data[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "        # Add a Ticker column\n",
    "        data[\"Ticker\"] = stock\n",
    "        # Save the data to CSV\n",
    "        data.to_csv(output_path, index=False)\n",
    "        # Reload and remove the first row (if needed)\n",
    "        data = pd.read_csv(output_path)\n",
    "        data = data.iloc[1:]\n",
    "        data.to_csv(output_path, index=False)\n",
    "\n",
    "    for stock in stocks:\n",
    "        data = yf.download(stock, start=\"2015-01-01\", end=\"2025-01-01\")\n",
    "        output_path = os.path.join(output_dir, f\"{stock}.csv\")\n",
    "        save_cleaned_data(data, stock, output_path)\n",
    "        # print(f\"Saved cleaned data for {stock} at {output_path}\")\n",
    "\n",
    "def preprocess_data(stock):\n",
    "    \"\"\"\n",
    "    Preprocess each stock's data by converting dates, sorting, and adding new features.\n",
    "    The processed CSV files are saved into a separate folder.\n",
    "    \"\"\"\n",
    "    data_dir = \"raw_stock_data\"\n",
    "    output_dir = \"processed_stock_data\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def preprocess_stock_data(file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        # Convert Date to datetime and sort\n",
    "        data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "        data.sort_values(by=\"Date\", inplace=True)\n",
    "        # Calculate additional features\n",
    "        data[\"Daily Return\"] = data[\"Close\"].pct_change()\n",
    "        data[\"5-Day Moving Avg\"] = data[\"Close\"].rolling(window=5).mean()\n",
    "        data[\"10-Day Volatility\"] = data[\"Close\"].rolling(window=10).std()\n",
    "        # Remove rows with NaN values from rolling calculations\n",
    "        data.dropna(inplace=True)\n",
    "        return data\n",
    "\n",
    "    for file_name in os.listdir(data_dir):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            stock_file_path = os.path.join(data_dir, file_name)\n",
    "            stock_name = os.path.splitext(file_name)[0]\n",
    "            # print(f\"Processing data for {stock_name}...\")\n",
    "            processed_data = preprocess_stock_data(stock_file_path)\n",
    "            output_path = os.path.join(output_dir, f\"{stock_name}_processed.csv\")\n",
    "            processed_data.to_csv(output_path, index=False)\n",
    "            # print(f\"Saved processed data for {stock_name} to {output_path}\")\n",
    "    # print(\"Data preprocessing complete!\")\n",
    "\n",
    "def load_stock_data(file_path):\n",
    "    \"\"\"\n",
    "    Load processed data from a CSV file and prepare features and target.\n",
    "    The target is the next day's closing price.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    features = data[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"5-Day Moving Avg\", \"10-Day Volatility\"]]\n",
    "    target = data[\"Close\"].shift(-1)\n",
    "    # Drop the last row (where target is NaN)\n",
    "    features = features[:-1]\n",
    "    target = target[:-1]\n",
    "    return features, target\n",
    "\n",
    "def train_model(ticker):\n",
    "    \"\"\"\n",
    "    Load the AAPL processed data, split into training and testing sets,\n",
    "    build and train a neural network with LSTM layers, and save the model.\n",
    "    \"\"\"\n",
    "    processed_data_dir = \"processed_stock_data\"\n",
    "    stock_file_path = os.path.join(processed_data_dir, f\"{ticker}_processed.csv\")\n",
    "    features, target = load_stock_data(stock_file_path)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features.values, target.values, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Reshape data for LSTM: (samples, timesteps, features)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "    # print(\"Training the model...\")\n",
    "    model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1)\n",
    "\n",
    "    # print(\"Evaluating the model on the test set...\")\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "    # print(f\"Test Loss: {loss}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    joblib.dump(model,f\"models/{ticker}_model.joblib\")\n",
    "    # model.save(\"models/AAPL_model.h5\")\n",
    "    # print(\"Model saved as 'models/AAPL_model.h5'\")\n",
    "\n",
    "def evaluate_model():\n",
    "    \"\"\"\n",
    "    Evaluate the saved model on a portion of AAPL's data and # print error metrics.\n",
    "    Also, predict the next few days' closing prices and save these predictions to CSV.\n",
    "    \"\"\"\n",
    "    processed_data_dir = \"processed_stock_data\"\n",
    "    model_path = \"stock_price_predictor.h5\"\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    def load_evaluation_data(file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        features = data[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"5-Day Moving Avg\", \"10-Day Volatility\"]]\n",
    "        target = data[\"Close\"].shift(-1)\n",
    "        features = features[:-1]\n",
    "        target = target[:-1]\n",
    "        # Use the last 10 rows for evaluation (e.g., next days' predictions)\n",
    "        evaluation_features = features[-10:]\n",
    "        evaluation_target = target[-10:]\n",
    "        # Remove the evaluation portion from the main dataset\n",
    "        features = features[:-10]\n",
    "        target = target[:-10]\n",
    "        return features, target, evaluation_features, evaluation_target\n",
    "\n",
    "    stock_file_path = os.path.join(processed_data_dir, \"AAPL_processed.csv\")\n",
    "    features, target, eval_features, eval_target = load_evaluation_data(stock_file_path)\n",
    "\n",
    "    # Reshape data for LSTM input\n",
    "    X_train = np.reshape(features.values, (features.shape[0], features.shape[1], 1))\n",
    "    X_eval = np.reshape(eval_features.values, (eval_features.shape[0], eval_features.shape[1], 1))\n",
    "\n",
    "    # Evaluate on the training portion (without the last few evaluation rows)\n",
    "    predicted_train = model.predict(X_train)\n",
    "    mse = mean_squared_error(target, predicted_train)\n",
    "    mae = mean_absolute_error(target, predicted_train)\n",
    "    r2 = r2_score(target, predicted_train)\n",
    "    \n",
    "    # print(\"### Model Evaluation on Training Data ###\")\n",
    "    # print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    # print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    # print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "    # Predict the next days' prices using the evaluation features\n",
    "    predicted_eval = model.predict(X_eval)\n",
    "    results = pd.DataFrame({\n",
    "        \"Index\": eval_features.index,\n",
    "        \"Actual Price\": eval_target.values,\n",
    "        \"Predicted Price\": predicted_eval.flatten()\n",
    "    })\n",
    "\n",
    "    # print(\"### Next Days' Predictions ###\")\n",
    "    # print(results)\n",
    "\n",
    "    # Save predictions to a CSV file\n",
    "    # results.to_csv(\"next_month_predictions.csv\", index=False)\n",
    "    # print(\"Predictions saved as 'next_month_predictions.csv'\")\n",
    "\n",
    "\n",
    "    # print(\"Starting the stock data pipeline...\")\n",
    "    download_and_clean_stock_data(ticker)\n",
    "    preprocess_data(ticker)\n",
    "    train_model(ticker)\n",
    "    # evaluate_model()\n",
    "    # print(\"Pipeline complete!\")\n",
    "\n",
    "\n",
    "for ticker in [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\", \"NVDA\", \"TSLA\"]:\n",
    "    main(ticker)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
